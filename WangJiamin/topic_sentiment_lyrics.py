# -*- coding: utf-8 -*-
"""Topic Sentiment Lyrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11FlhuOSlVElRUIHe5GWYlnliumLH4q2Q
"""

!pip install nltk gensim scikit-learn shap

import pandas as pd
import nltk
import gensim
from gensim import corpora
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import string
nltk.download('stopwords')
nltk.download('vader_lexicon')

import torch
import shap
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.utils import resample
from sklearn.preprocessing import StandardScaler
from nltk.sentiment import SentimentIntensityAnalyzer
from scipy.stats import chi2_contingency

# Read the dataset
song_lyrics = pd.read_csv(
    '/content/filtered_song_lyrics.csv',
    sep=',',                 # Identify comma-separated
    quotechar='"',           # If there are commas inside the lyrics, wrap them in double quotes.
    doublequote=True,        # Allow continuous quotation marks in lyrics
    escapechar='\\',         # Prevent strange escape characters in the lyrics
    engine='python',         # Using python engine
    on_bad_lines='skip'      # Skip when encountering bad things
)

# clean
song_lyrics['lyrics'] = song_lyrics['lyrics'].fillna('').str.strip()

# Simple tokenization preprocessing
def preprocess(text):
    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    return tokens

song_lyrics['tokens'] = song_lyrics['lyrics'].apply(preprocess)

# Preparation of the dictionary and corpus
dictionary = corpora.Dictionary(song_lyrics['tokens'])
corpus = [dictionary.doc2bow(text) for text in song_lyrics['tokens']]

# Training the LDA model (with 50 topics)
lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, random_state=42, passes=10, per_word_topics=True)

# Extracting the topic distribution vector for each song lyric
def get_topic_vector(bow):
    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0)
    topic_vector = [prob for _, prob in topic_dist]
    return topic_vector

song_lyrics['topic_vector'] = song_lyrics['tokens'].apply(lambda tokens: get_topic_vector(dictionary.doc2bow(tokens)))

# Splitting it into a DataFrame
topic_df = pd.DataFrame(song_lyrics['topic_vector'].to_list(), columns=[f"Topic_{i}" for i in range(50)])
# Merging it back
final_df = pd.concat([song_lyrics, topic_df], axis=1)

sia = SentimentIntensityAnalyzer()

def get_sentiment(text):
    if not text or text.strip() == "":
        return None
    score = sia.polarity_scores(text)
    compound = score['compound']
    if compound >= 0.6:
        return 1
    elif compound <= -0.6:
        return 0
    else:
        return None

final_df['sentiment_label'] = final_df['lyrics'].apply(get_sentiment)
final_df = final_df.dropna(subset=['sentiment_label'])
final_df['sentiment_label'] = final_df['sentiment_label'].astype(int)

genre_onehot = pd.get_dummies(final_df['tag'], prefix='genre')
scaler = StandardScaler()
year_scaled = scaler.fit_transform(final_df[['year']])
year_scaled_df = pd.DataFrame(year_scaled, columns=['year_scaled'])

feature_df = pd.concat([
    final_df[[f'Topic_{i}' for i in range(50)]].reset_index(drop=True),
    genre_onehot.reset_index(drop=True),
    year_scaled_df.reset_index(drop=True)
], axis=1)

X = feature_df
y = final_df['sentiment_label'].reset_index(drop=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

def evaluate_model(y_true, y_pred, model_name):
    print(f"\n--- {model_name} ---")
    print(classification_report(y_true, y_pred))
    print(f"ROC AUC Score: {roc_auc_score(y_true, y_pred)}")

evaluate_model(y_test, y_pred_lr, "Logistic Regression")
evaluate_model(y_test, y_pred_svm, "SVM")

def bootstrap_accuracy(model, X, y, n_iterations=1000):
    scores = []
    for i in range(n_iterations):
        X_resampled, y_resampled = resample(X, y, random_state=i)
        model.fit(X_resampled, y_resampled)
        y_pred = model.predict(X_test)
        score = roc_auc_score(y_test, y_pred)
        scores.append(score)
    return np.percentile(scores, [2.5, 97.5])

ci_lr = bootstrap_accuracy(LogisticRegression(max_iter=1000, random_state=42), X_train, y_train)
ci_svm = bootstrap_accuracy(SVC(kernel='rbf', probability=True, random_state=42), X_train, y_train)

print(f"Logistic Regression 95% CI: {ci_lr}")
print(f"SVM 95% CI: {ci_svm}")

final_df['dominant_topic'] = final_df[[f'Topic_{i}' for i in range(50)]].idxmax(axis=1)
contingency_table = pd.crosstab(final_df['dominant_topic'], final_df['sentiment_label'])
chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square Statistic: {chi2}")
print(f"p-value: {p}")

y_prob_lr = lr_model.predict_proba(X_test)[:,1]
y_prob_svm = svm_model.predict_proba(X_test)[:,1]

fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)
fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)

plt.figure(figsize=(8,6))
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC={auc(fpr_lr,tpr_lr):.2f})')
plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC={auc(fpr_svm,tpr_svm):.2f})')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

topic_sentiment_table = pd.crosstab(final_df['dominant_topic'], final_df['sentiment_label'])

plt.figure(figsize=(12,8))
sns.heatmap(topic_sentiment_table, annot=True, fmt="d", cmap="YlGnBu")
plt.title('Dominant Topic vs Sentiment Label')
plt.show()

explainer = shap.Explainer(lr_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar", max_display=20)